{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# import libraries\n",
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sklearny\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "# RNN - LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "id": "YeuAheYyhdZw",
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.088471Z",
     "start_time": "2025-03-23T07:50:31.081308Z"
    }
   },
   "outputs": [],
   "execution_count": 349
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Input dataset before model training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.103545Z",
     "start_time": "2025-03-23T07:50:31.100085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'needn', 'because', 'up', \"she'll\", 'or', \"you've\", \"shan't\", 'over', 'where', 'wasn', 'into', 'out', \"i'll\", 'that', 'yourself', 'by', 'same', 'at', 'before', 'hasn', 'll', 'mustn', 'don', 'when', 'it', 'hadn', 'very', \"they'd\", 'you', 'after', 'the', \"you'd\", 'their', 'those', \"mightn't\", 'which', 'only', 'what', 'i', 'has', 'will', 'can', 'each', 'she', 're', \"that'll\", \"they've\", 'no', 'this', \"hasn't\", \"he'd\", 'down', \"needn't\", 'isn', 'm', 'her', 'between', 'whom', 'such', 'is', 'just', 'hers', \"hadn't\", 'haven', \"i'm\", 'and', 'most', 'own', \"they're\", \"i've\", 'there', 'during', 'through', \"he's\", 'ours', 'doing', 'am', 'mightn', \"it'll\", 'we', 'how', 'itself', 'y', 'our', 'any', 'once', 'few', 'wouldn', 'are', 'd', 'these', 'as', 'ma', 'himself', 'both', \"couldn't\", 'having', 'me', 'than', \"it's\", 'o', \"you'll\", 'be', \"doesn't\", 'not', 'some', \"we're\", \"won't\", 't', 's', 'had', 'against', \"i'd\", 'aren', 'should', 'nor', 'have', 'further', 'with', 'a', 'too', \"aren't\", 'them', 'were', 'ourselves', 'theirs', \"we'll\", 'but', 'off', 'for', 'they', 'doesn', \"wouldn't\", 'themselves', 'was', 'yours', 'yourselves', 'in', 'about', 'then', 'until', 'again', 'shan', 'him', 'of', \"we've\", 'its', \"didn't\", 'myself', \"should've\", \"isn't\", 'while', 'your', 'why', 'so', 'an', \"it'd\", 'didn', 'on', \"you're\", 'above', 'couldn', \"shouldn't\", 'does', \"haven't\", 'if', 'being', 'his', 'from', 'other', 'more', \"weren't\", 'do', 'my', 'under', 'did', 'now', 'herself', \"we'd\", 'who', 'here', 've', 'shouldn', 'below', \"she'd\", 'been', \"mustn't\", \"they'll\", \"wasn't\", 'weren', 'won', 'he', \"he'll\", \"don't\", 'to', 'ain', \"she's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhaijingjing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 350
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.130654Z",
     "start_time": "2025-03-23T07:50:31.127192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negation_words = {\"no\", \"nor\", \"not\", \"don\", \"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"can't\", \"cannot\", \"won't\", \"wouldn\", \"wouldn't\", \"shouldn\", \"shouldn't\", \"isn't\", \"aren't\", \"wasn\", \"wasn't\", \"weren't\", \"hadn\", \"hadn't\", \"\", \"wouldn't\"}\n",
    "custom_stop_words = stopword.difference(negation_words)\n",
    "print(custom_stop_words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'needn', 'because', 'up', \"she'll\", 'or', \"you've\", \"shan't\", 'over', 'where', 'into', 'out', \"i'll\", 'that', 'yourself', 'by', 'same', 'at', 'before', 'hasn', 'll', 'mustn', 'when', 'it', 'very', \"they'd\", 'you', 'after', 'the', \"you'd\", 'their', 'those', \"mightn't\", 'which', 'only', 'what', 'i', 'has', 'will', 'can', 'each', 'she', 're', \"that'll\", \"they've\", 'this', \"hasn't\", \"he'd\", 'down', \"needn't\", 'isn', 'm', 'her', 'between', 'whom', 'such', 'is', 'just', 'hers', 'haven', \"i'm\", 'and', 'most', 'own', \"they're\", \"i've\", 'there', 'during', 'through', \"he's\", 'ours', 'doing', 'am', 'mightn', \"it'll\", 'we', 'how', 'itself', 'y', 'our', 'any', 'once', 'few', 'are', 'd', 'these', 'as', 'ma', 'himself', 'both', \"couldn't\", 'having', 'me', 'than', \"it's\", 'o', \"you'll\", 'be', 'some', \"we're\", 't', 's', 'had', 'against', \"i'd\", 'aren', 'should', 'have', 'further', 'with', 'a', 'too', 'them', 'were', 'ourselves', 'theirs', \"we'll\", 'but', 'off', 'for', 'they', 'themselves', 'was', 'yours', 'yourselves', 'in', 'about', 'then', 'until', 'again', 'shan', 'him', 'of', \"we've\", 'its', 'myself', \"should've\", 'while', 'your', 'why', 'so', 'an', \"it'd\", 'on', \"you're\", 'above', 'couldn', 'does', \"haven't\", 'if', 'being', 'his', 'from', 'other', 'more', 'do', 'my', 'under', 'did', 'now', 'herself', \"we'd\", 'who', 'here', 've', 'below', \"she'd\", 'been', \"mustn't\", \"they'll\", 'weren', 'won', 'he', \"he'll\", 'to', 'ain', \"she's\"}\n"
     ]
    }
   ],
   "execution_count": 351
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.206365Z",
     "start_time": "2025-03-23T07:50:31.195419Z"
    }
   },
   "cell_type": "code",
   "source": "{'all', 'needn', 'because', 'up', \"she'll\", 'or', \"you've\", \"shan't\", 'over', 'where', 'wasn', 'into', 'out', \"i'll\", 'that', 'yourself', 'by', 'same', 'at', 'before', 'hasn', 'll', 'mustn', 'don', 'when', 'it', 'hadn', 'very', \"they'd\", 'you', 'after', 'the', \"you'd\", 'their', 'those', \"mightn't\", 'which', 'only', 'what', 'i', 'has', 'will', 'can', 'each', 'she', 're', \"that'll\", \"they've\", 'no', 'this', \"hasn't\", \"he'd\", 'down', \"needn't\", 'isn', 'm', 'her', 'between', 'whom', 'such', 'is', 'just', 'hers', \"hadn't\", 'haven', \"i'm\", 'and', 'most', 'own', \"they're\", \"i've\", 'there', 'during', 'through', \"he's\", 'ours', 'doing', 'am', 'mightn', \"it'll\", 'we', 'how', 'itself', 'y', 'our', 'any', 'once', 'few', 'wouldn', 'are', 'd', 'these', 'as', 'ma', 'himself', 'both', \"couldn't\", 'having', 'me', 'than', \"it's\", 'o', \"you'll\", 'be', \"doesn't\", 'not', 'some', \"we're\", \"won't\", 't', 's', 'had', 'against', \"i'd\", 'aren', 'should', 'nor', 'have', 'further', 'with', 'a', 'too', \"aren't\", 'them', 'were', 'ourselves', 'theirs', \"we'll\", 'but', 'off', 'for', 'they', 'doesn', \"wouldn't\", 'themselves', 'was', 'yours', 'yourselves', 'in', 'about', 'then', 'until', 'again', 'shan', 'him', 'of', \"we've\", 'its', \"didn't\", 'myself', \"should've\", \"isn't\", 'while', 'your', 'why', 'so', 'an', \"it'd\", 'didn', 'on', \"you're\", 'above', 'couldn', \"shouldn't\", 'does', \"haven't\", 'if', 'being', 'his', 'from', 'other', 'more', \"weren't\", 'do', 'my', 'under', 'did', 'now', 'herself', \"we'd\", 'who', 'here', 've', 'shouldn', 'below', \"she'd\", 'been', \"mustn't\", \"they'll\", \"wasn't\", 'weren', 'won', 'he', \"he'll\", \"don't\", 'to', 'ain', \"she's\"}",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 352
  },
  {
   "cell_type": "code",
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "punctuationPattern = r\"[^\\w\\s']|(?<!\\w)n\\'t\"\n",
    "def process_tweets_not_del_stopw(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    # tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    # final_tokens = [w for w in tokens if w not in stopword]\n",
    "    final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_negation_append_del_stopw(tweet):\n",
    "    # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    # tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    tweet = re.sub(punctuationPattern,'',tweet)\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    # Remove punctuations but remain ‘n't’\n",
    "    final_tokens = tokens\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    prev_neg = False\n",
    "\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        #finalwords.append(word)\n",
    "        if word in negation_words:\n",
    "          prev_neg = True\n",
    "        elif prev_neg:\n",
    "          finalwords.append(\"not_\" + word)\n",
    "          prev_neg = False\n",
    "        else:\n",
    "          finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_negation_append(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    # tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    tweet = re.sub(punctuationPattern,'',tweet)\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    # Remove punctuations but remain ‘n't’\n",
    "    final_tokens = tokens\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    prev_neg = False\n",
    "\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        #finalwords.append(word)\n",
    "        if word in negation_words:\n",
    "          prev_neg = True\n",
    "        elif prev_neg:\n",
    "          finalwords.append(\"not_\" + word)\n",
    "          prev_neg = False\n",
    "        else:\n",
    "          finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_remain_negation(tweet):\n",
    "    # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    print(f\"Remove punctuations: {tweet}\")\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    negation_words = {\"no\", \"nor\", \"not\", \"don\", \"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"can't\", \"cannot\", \"won't\", \"wouldn\", \"wouldn't\", \"shouldn\", \"shouldn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"}\n",
    "\n",
    "    # Remove negation words from the stop word list\n",
    "    custom_stop_words = stopword.difference(negation_words)\n",
    "    # print(f\"custom_stop_words: {custom_stop_words}\")\n",
    "\n",
    "    final_tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_del_stopw(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    # tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.259238Z",
     "start_time": "2025-03-23T07:50:31.247332Z"
    }
   },
   "outputs": [],
   "execution_count": 353
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenizer(max_words, test_text):\n",
    "    # 1. build word table with top 50 words\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(test_text)\n",
    "    print(\"单词表:\", tokenizer.word_index)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(test_text)\n",
    "    print(\"文本的单词索引序列:\", sequences)\n",
    "\n",
    "    tweets = pad_sequences(sequences, maxlen=10)\n",
    "    print(tweets.shape)\n",
    "    print(\"填充后文本的单词索引序列:\", tweets)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.292394Z",
     "start_time": "2025-03-23T07:50:31.289273Z"
    }
   },
   "outputs": [],
   "execution_count": 354
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "max_words = 100\n",
    "test_tweets = [\n",
    "    \"Aww... @Wowena just got home from church??? He doesn't go today\",\n",
    "    \"You aren't happy with the service at all\",\n",
    "    \"i'll never come back! website at https://www.example.com\",\n",
    "    \"Not happy with the service at all.\"\n",
    "]\n",
    "\n",
    "processed_test_text = [process_tweets_negation_append_del_stopw(x) for x in test_tweets]\n",
    "print(\"预处理后的数据:\", processed_test_text)\n",
    "tokenizer(max_words, processed_test_text)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T07:50:31.328524Z",
     "start_time": "2025-03-23T07:50:31.323747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['aww', 'just', 'got', 'home', 'from', 'church', 'he', 'does', \"n't\", 'go', 'today']\n",
      "tokens: ['you', 'are', \"n't\", 'happy', 'with', 'the', 'service', 'at', 'all']\n",
      "tokens: ['i', \"'ll\", 'never', 'come', 'back', 'website', 'at']\n",
      "tokens: ['not', 'happy', 'with', 'the', 'service', 'at', 'all']\n",
      "预处理后的数据: ['aww just got home from church he doe not_go today', 'you are not_happy with the service at all', \"'ll not_come back website at\", 'not_happy with the service at all']\n",
      "单词表: {'not': 1, 'at': 2, 'happy': 3, 'with': 4, 'the': 5, 'service': 6, 'all': 7, 'aww': 8, 'just': 9, 'got': 10, 'home': 11, 'from': 12, 'church': 13, 'he': 14, 'doe': 15, 'go': 16, 'today': 17, 'you': 18, 'are': 19, \"'ll\": 20, 'come': 21, 'back': 22, 'website': 23}\n",
      "文本的单词索引序列: [[8, 9, 10, 11, 12, 13, 14, 15, 1, 16, 17], [18, 19, 1, 3, 4, 5, 6, 2, 7], [20, 1, 21, 22, 23, 2], [1, 3, 4, 5, 6, 2, 7]]\n",
      "(4, 10)\n",
      "填充后文本的单词索引序列: [[ 9 10 11 12 13 14 15  1 16 17]\n",
      " [ 0 18 19  1  3  4  5  6  2  7]\n",
      " [ 0  0  0  0 20  1 21 22 23  2]\n",
      " [ 0  0  0  1  3  4  5  6  2  7]]\n"
     ]
    }
   ],
   "execution_count": 355
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNag92UMCy1GtgfJ+zFFU1",
   "collapsed_sections": [],
   "name": "DUDL_FFN_aboutMNIST.ipynb",
   "provenance": [
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "interpreter": {
   "hash": "f5c9dfabb21bd2a91b63810df81acbdc6b5e617e45414f0ef050ca96090c868f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
