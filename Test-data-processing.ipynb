{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# import libraries\n",
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sklearny\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "# RNN - LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "outputs": [],
   "metadata": {
    "id": "YeuAheYyhdZw",
    "ExecuteTime": {
     "end_time": "2025-03-22T21:23:56.500059Z",
     "start_time": "2025-03-22T21:23:56.491892Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Input dataset before model training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'they', \"he'd\", 'against', \"you'll\", 'all', 'same', \"mustn't\", 'ma', 'myself', 'are', 'of', 'am', \"it'd\", 'there', 'not', 'y', 'had', 'their', 'own', 'having', 'haven', 'very', \"doesn't\", 'm', 'only', 'shouldn', 'below', 'off', 'did', 's', \"he'll\", 'have', 'other', 'been', 'until', 'if', 'yourself', 'up', 'an', 'we', 'herself', \"that'll\", 'his', 'has', \"you're\", 'and', 'mustn', 'my', 'out', 'it', 'then', \"we've\", \"hadn't\", 'i', 'do', \"i'd\", 'you', 'her', \"aren't\", 'those', 'won', 'each', 'that', 'its', 'your', 'him', 'because', 'but', \"isn't\", 'd', 'after', 'why', 'will', \"haven't\", 'nor', \"they'll\", 'on', 'them', 'be', 'should', \"weren't\", 'between', 'a', 'themselves', 'were', 'll', 'weren', 'he', 'who', 'about', 'our', \"didn't\", 'how', 'above', 'wasn', 'yours', \"couldn't\", \"hasn't\", 'most', 'through', 'as', 'didn', 're', \"should've\", 'the', 'isn', 'yourselves', 'is', 'too', 'ain', 'aren', \"it'll\", 'some', 'few', 'o', \"mightn't\", 'hers', 'no', 'being', 'was', 'over', 'mightn', 'doesn', \"she'll\", \"wasn't\", \"she'd\", 'such', 'can', 'again', \"i've\", \"they've\", 'hadn', \"shouldn't\", 'himself', 'where', 'by', 'me', 'or', 'both', 'couldn', 've', \"it's\", 'for', 'which', 'don', \"she's\", 'than', \"they're\", 'these', 'when', 'hasn', 'from', 'ourselves', \"i'm\", 'itself', 'theirs', \"you'd\", 'whom', 'during', 't', 'shan', 'this', 'more', \"needn't\", 'so', \"won't\", \"he's\", \"you've\", 'once', 'just', 'now', 'at', 'further', 'with', 'to', 'in', 'into', 'needn', 'before', \"shan't\", 'does', 'down', \"i'll\", 'here', 'under', 'ours', 'she', \"don't\", 'while', 'wouldn', 'what', \"they'd\", \"we'll\", 'doing', \"we'd\", 'any', \"we're\", \"wouldn't\"}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiachen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T21:23:56.509748Z",
     "start_time": "2025-03-22T21:23:56.506375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    negation_words = {\"no\", \"nor\", \"not\", \"don\", \"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"can't\", \"cannot\", \"won't\", \"wouldn\", \"wouldn't\", \"shouldn\", \"shouldn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"}\n",
    "\n",
    "    # Remove negation words from the stop word list\n",
    "    custom_stop_words = stopword.difference(negation_words)\n",
    "    final_tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    #final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_append(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    #final_tokens = [w for w in tokens if w not in stopword]\n",
    "    final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    prev_neg = False\n",
    "\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        #finalwords.append(word)\n",
    "        if word in negation_words:\n",
    "          prev_neg = True\n",
    "        elif prev_neg:\n",
    "          finalwords.append(\"not_\" + word)\n",
    "          prev_neg = False\n",
    "        else:\n",
    "          finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T21:23:56.529Z",
     "start_time": "2025-03-22T21:23:56.523358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def tokenizer(max_words, test_text):\n",
    "    # 1. build word table with top 50 words\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(test_text)\n",
    "    print(\"单词表:\", tokenizer.word_index)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(test_text)\n",
    "    print(\"文本的单词索引序列:\", sequences)\n",
    "\n",
    "    tweets = pad_sequences(sequences, maxlen=10)\n",
    "    print(tweets.shape)\n",
    "    print(\"填充后文本的单词索引序列:\", tweets)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T21:23:56.552501Z",
     "start_time": "2025-03-22T21:23:56.549456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Test\n",
    "max_words = 20\n",
    "test_tweets = [\n",
    "    \"I LOVE the movie!!!@user123\",\n",
    "    \"Not happy with the service at all\",\n",
    "    \"Never coming back! website at https://www.example.com\",\n",
    "    \"Not happy with the service at all.\"\n",
    "]\n",
    "\n",
    "processed_test_text = [process_tweets(x) for x in test_tweets]\n",
    "print(\"预处理后的数据:\", processed_test_text)\n",
    "tokenizer(max_words, processed_test_text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "预处理后的数据: ['love movie', 'not happy service', 'never coming back website', 'not happy service']\n",
      "单词表: {'not': 1, 'happy': 2, 'service': 3, 'love': 4, 'movie': 5, 'never': 6, 'coming': 7, 'back': 8, 'website': 9}\n",
      "文本的单词索引序列: [[4, 5], [1, 2, 3], [6, 7, 8, 9], [1, 2, 3]]\n",
      "(4, 10)\n",
      "填充后文本的单词索引序列: [[0 0 0 0 0 0 0 0 4 5]\n",
      " [0 0 0 0 0 0 0 1 2 3]\n",
      " [0 0 0 0 0 0 6 7 8 9]\n",
      " [0 0 0 0 0 0 0 1 2 3]]\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T21:23:56.608082Z",
     "start_time": "2025-03-22T21:23:56.604641Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNag92UMCy1GtgfJ+zFFU1",
   "collapsed_sections": [],
   "name": "DUDL_FFN_aboutMNIST.ipynb",
   "provenance": [
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit ('myenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "interpreter": {
   "hash": "f5c9dfabb21bd2a91b63810df81acbdc6b5e617e45414f0ef050ca96090c868f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}