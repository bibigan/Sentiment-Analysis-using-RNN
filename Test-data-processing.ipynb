{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# import libraries\n",
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sklearny\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "# RNN - LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "id": "YeuAheYyhdZw",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.771806Z",
     "start_time": "2025-03-23T09:51:34.763096Z"
    }
   },
   "outputs": [],
   "execution_count": 579
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Input dataset before model training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.789012Z",
     "start_time": "2025-03-23T09:51:34.782706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'needn', 'because', 'up', \"she'll\", 'or', \"you've\", \"shan't\", 'over', 'where', 'wasn', 'into', 'out', \"i'll\", 'that', 'yourself', 'by', 'same', 'at', 'before', 'hasn', 'll', 'mustn', 'don', 'when', 'it', 'hadn', 'very', \"they'd\", 'you', 'after', 'the', \"you'd\", 'their', 'those', \"mightn't\", 'which', 'only', 'what', 'i', 'has', 'will', 'can', 'each', 'she', 're', \"that'll\", \"they've\", 'no', 'this', \"hasn't\", \"he'd\", 'down', \"needn't\", 'isn', 'm', 'her', 'between', 'whom', 'such', 'is', 'just', 'hers', \"hadn't\", 'haven', \"i'm\", 'and', 'most', 'own', \"they're\", \"i've\", 'there', 'during', 'through', \"he's\", 'ours', 'doing', 'am', 'mightn', \"it'll\", 'we', 'how', 'itself', 'y', 'our', 'any', 'once', 'few', 'wouldn', 'are', 'd', 'these', 'as', 'ma', 'himself', 'both', \"couldn't\", 'having', 'me', 'than', \"it's\", 'o', \"you'll\", 'be', \"doesn't\", 'not', 'some', \"we're\", \"won't\", 't', 's', 'had', 'against', \"i'd\", 'aren', 'should', 'nor', 'have', 'further', 'with', 'a', 'too', \"aren't\", 'them', 'were', 'ourselves', 'theirs', \"we'll\", 'but', 'off', 'for', 'they', 'doesn', \"wouldn't\", 'themselves', 'was', 'yours', 'yourselves', 'in', 'about', 'then', 'until', 'again', 'shan', 'him', 'of', \"we've\", 'its', \"didn't\", 'myself', \"should've\", \"isn't\", 'while', 'your', 'why', 'so', 'an', \"it'd\", 'didn', 'on', \"you're\", 'above', 'couldn', \"shouldn't\", 'does', \"haven't\", 'if', 'being', 'his', 'from', 'other', 'more', \"weren't\", 'do', 'my', 'under', 'did', 'now', 'herself', \"we'd\", 'who', 'here', 've', 'shouldn', 'below', \"she'd\", 'been', \"mustn't\", \"they'll\", \"wasn't\", 'weren', 'won', 'he', \"he'll\", \"don't\", 'to', 'ain', \"she's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhaijingjing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 580
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.812572Z",
     "start_time": "2025-03-23T09:51:34.808460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negation_words = {\"no\", \"nor\", \"not\", \"don\", \"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"can't\", \"cannot\", \"won't\", \"wouldn\", \"wouldn't\", \"shouldn\", \"shouldn't\", \"isn't\", \"aren't\", \"wasn\", \"wasn't\", \"weren't\", \"hadn\", \"hadn't\", \"\", \"wouldn't\"}\n",
    "custom_stop_words = stopword.difference(negation_words)\n",
    "print(custom_stop_words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'needn', 'because', 'up', \"she'll\", 'or', \"you've\", \"shan't\", 'over', 'where', 'into', 'out', \"i'll\", 'that', 'yourself', 'by', 'same', 'at', 'before', 'hasn', 'll', 'mustn', 'when', 'it', 'very', \"they'd\", 'you', 'after', 'the', \"you'd\", 'their', 'those', \"mightn't\", 'which', 'only', 'what', 'i', 'has', 'will', 'can', 'each', 'she', 're', \"that'll\", \"they've\", 'this', \"hasn't\", \"he'd\", 'down', \"needn't\", 'isn', 'm', 'her', 'between', 'whom', 'such', 'is', 'just', 'hers', 'haven', \"i'm\", 'and', 'most', 'own', \"they're\", \"i've\", 'there', 'during', 'through', \"he's\", 'ours', 'doing', 'am', 'mightn', \"it'll\", 'we', 'how', 'itself', 'y', 'our', 'any', 'once', 'few', 'are', 'd', 'these', 'as', 'ma', 'himself', 'both', \"couldn't\", 'having', 'me', 'than', \"it's\", 'o', \"you'll\", 'be', 'some', \"we're\", 't', 's', 'had', 'against', \"i'd\", 'aren', 'should', 'have', 'further', 'with', 'a', 'too', 'them', 'were', 'ourselves', 'theirs', \"we'll\", 'but', 'off', 'for', 'they', 'themselves', 'was', 'yours', 'yourselves', 'in', 'about', 'then', 'until', 'again', 'shan', 'him', 'of', \"we've\", 'its', 'myself', \"should've\", 'while', 'your', 'why', 'so', 'an', \"it'd\", 'on', \"you're\", 'above', 'couldn', 'does', \"haven't\", 'if', 'being', 'his', 'from', 'other', 'more', 'do', 'my', 'under', 'did', 'now', 'herself', \"we'd\", 'who', 'here', 've', 'below', \"she'd\", 'been', \"mustn't\", \"they'll\", 'weren', 'won', 'he', \"he'll\", 'to', 'ain', \"she's\"}\n"
     ]
    }
   ],
   "execution_count": 581
  },
  {
   "cell_type": "code",
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "# 去除标点符号，保留'\n",
    "punctuationPattern = r\"[^\\w\\s']|(?<!\\w)n\\'t\"\n",
    "def process_tweets_not_del_stopw(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    # tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    # final_tokens = [w for w in tokens if w not in stopword]\n",
    "    final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_negation_del_stopword(tweet):\n",
    "    # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    # 移除非字母数字、空格和'的字符\n",
    "    tweet = re.sub(r\"[^\\w\\s']\", '', tweet)\n",
    "    # --- 特殊否定缩写处理 ---\n",
    "    # 1. 明确处理 can't → cannot\n",
    "    tweet = re.sub(r\"\\bcan't\\b\", \"cannot\", tweet)\n",
    "    # 2. 处理 won't → will not\n",
    "    tweet = re.sub(r\"\\bwon't\\b\", \"will not\", tweet)\n",
    "    # 3. 处理 shan't → shall not\n",
    "    tweet = re.sub(r\"\\bshan't\\b\", \"shall not\", tweet)\n",
    "    # 4. 处理 ain't → is not\n",
    "    tweet = re.sub(r\"\\bain't\\b\", \"is not\", tweet)\n",
    "    # --- 通用n't处理 ---\n",
    "    # 将剩余n't替换为 not（如didn't → did not）\n",
    "    tweet = re.sub(r\"n't\\b\", \" not\", tweet)\n",
    "    # --- 处理其他撇号为空格（如I'll → I ll）---\n",
    "    tweet = re.sub(r\"'\", ' ', tweet)\n",
    "    print(f\"tweet: {tweet}\")\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    print(f\"before removing stopword tokens: {tokens}\")\n",
    "\n",
    "    # 移除非否定停用词\n",
    "    negation_words = {\n",
    "        # 基础否定词\n",
    "        \"not\", \"no\", \"never\", \"nor\", \"none\", \"nobody\", \"nothing\",\n",
    "        \"neither\", \"nowhere\", \"without\", \"cannot\"\n",
    "        # 预处理后可能保留的否定词（如来自特殊缩写替换）\n",
    "        # ,\"isn\", \"aren\", \"wasn\", \"weren\"\n",
    "    }\n",
    "    custom_stop_words = stopword.difference(negation_words)\n",
    "    final_tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    print(f\"after removing stopword tokens: {final_tokens}\")\n",
    "\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "\n",
    "    for w in final_tokens:\n",
    "        if len(w) > 1:\n",
    "            # 动词词性还原（解决 does→doe 问题）\n",
    "            word = wordLemm.lemmatize(w, pos='v')\n",
    "            # 名词词性还原（根据任务需求）\n",
    "            word = wordLemm.lemmatize(word, pos='n')\n",
    "            finalwords.append(word)\n",
    "    return finalwords\n",
    "\n",
    "def process_tweets_negation_append_fix(tweet):\n",
    "    # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    # 移除非字母数字、空格和'的字符\n",
    "    tweet = re.sub(r\"[^\\w\\s']\", '', tweet)\n",
    "    # --- 特殊否定缩写处理 ---\n",
    "    # 1. 明确处理 can't → cannot\n",
    "    tweet = re.sub(r\"\\bcan't\\b\", \"cannot\", tweet)\n",
    "    # 2. 处理 won't → will not\n",
    "    tweet = re.sub(r\"\\bwon't\\b\", \"will not\", tweet)\n",
    "    # 3. 处理 shan't → shall not\n",
    "    tweet = re.sub(r\"\\bshan't\\b\", \"shall not\", tweet)\n",
    "    # 4. 处理 ain't → is not\n",
    "    tweet = re.sub(r\"\\bain't\\b\", \"is not\", tweet)\n",
    "    # --- 通用n't处理 ---\n",
    "    # 将剩余n't替换为 not（如didn't → did not）\n",
    "    tweet = re.sub(r\"n't\\b\", \" not\", tweet)\n",
    "    # --- 处理其他撇号为空格（如I'll → I ll）---\n",
    "    tweet = re.sub(r\"'\", ' ', tweet)\n",
    "    print(f\"tweet: {tweet}\")\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    # Remove punctuations but remain ‘n't’\n",
    "    final_tokens = tokens\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "\n",
    "    for w in tokens:\n",
    "        if len(w) > 1:\n",
    "            # 动词词性还原（解决 does→doe 问题）\n",
    "            word = wordLemm.lemmatize(w, pos='v')\n",
    "            # 名词词性还原（根据任务需求）\n",
    "            word = wordLemm.lemmatize(word, pos='n')\n",
    "            finalwords.append(word)\n",
    "    return finalwords\n",
    "\n",
    "# 数据处理方法有点问题，tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "# 去除了标点符号，导致带有'n't'的否定词无法被识别为negative word，\n",
    "# 如何修复既能识别到带有'n't'的否定词，又能去除其它标点符号\n",
    "# 还需要注意类似I'll要改为I ll\n",
    "def process_tweets_negation_append(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    print(f\"tweet: {tweet}\")\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #Removing Stop Words\n",
    "    #final_tokens = [w for w in tokens if w not in stopword]\n",
    "    final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    prev_neg = False\n",
    "\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        #finalwords.append(word)\n",
    "        if word in negation_words:\n",
    "          prev_neg = True\n",
    "        elif prev_neg:\n",
    "          finalwords.append(\"not_\" + word)\n",
    "          prev_neg = False\n",
    "        else:\n",
    "          finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "# punctuations处理会让\"didn't\" 变成 \"didnt\"，这不在stopword的否定词（didn,didn't），可以留下\n",
    "# punctuations处理会让\"i'll\" 变成 \"ill\" 和其它单词混淆\n",
    "def process_tweets_remain_negation(tweet):\n",
    "    # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    #tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    print(f\"Remove punctuations: {tweet}\")\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    negation_words = {\"no\", \"nor\", \"not\", \"don\", \"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"can't\", \"cannot\", \"won't\", \"wouldn\", \"wouldn't\", \"shouldn\", \"shouldn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"}\n",
    "\n",
    "    # Remove negation words from the stop word list\n",
    "    custom_stop_words = stopword.difference(negation_words)\n",
    "    # print(f\"custom_stop_words: {custom_stop_words}\")\n",
    "\n",
    "    final_tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #final_tokens = tokens\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "def process_tweets_del_stopw(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    # tweet=tweet[1:]\n",
    "    # Removing all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    print(f\"tokens: {tokens}\")\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.875530Z",
     "start_time": "2025-03-23T09:51:34.856609Z"
    }
   },
   "outputs": [],
   "execution_count": 582
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenizer(max_words, test_text):\n",
    "    # 1. build word table with top 50 words\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(test_text)\n",
    "    print(\"单词表:\", tokenizer.word_index)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(test_text)\n",
    "    print(\"文本的单词索引序列:\", sequences)\n",
    "\n",
    "    tweets = pad_sequences(sequences, maxlen=10)\n",
    "    print(tweets.shape)\n",
    "    print(\"填充后文本的单词索引序列:\", tweets)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.887979Z",
     "start_time": "2025-03-23T09:51:34.885020Z"
    }
   },
   "outputs": [],
   "execution_count": 583
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "max_words = 100\n",
    "test_tweets = [\n",
    "    \"Aww... @Wowena just won't got home from church??? He doesn't go today\",\n",
    "    \"You aren't happy with the tables at all\",\n",
    "    \"i'll mightn't come back! website at https://www.example.com\",\n",
    "    \"They ain't happy with the service at all.\"\n",
    "]\n",
    "\n",
    "processed_test_text = [process_tweets_negation_del_stopword(x) for x in test_tweets]\n",
    "print(\"预处理后的数据:\", processed_test_text)\n",
    "tokenizer(max_words, processed_test_text)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:51:34.907177Z",
     "start_time": "2025-03-23T09:51:34.901367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet: aww  just will not got home from church he does not go today\n",
      "before removing stopword tokens: ['aww', 'just', 'will', 'not', 'got', 'home', 'from', 'church', 'he', 'does', 'not', 'go', 'today']\n",
      "after removing stopword tokens: ['aww', 'not', 'got', 'home', 'church', 'not', 'go', 'today']\n",
      "tweet: you are not happy with the tables at all\n",
      "before removing stopword tokens: ['you', 'are', 'not', 'happy', 'with', 'the', 'tables', 'at', 'all']\n",
      "after removing stopword tokens: ['not', 'happy', 'tables']\n",
      "tweet: i ll might not come back website at \n",
      "before removing stopword tokens: ['i', 'll', 'might', 'not', 'come', 'back', 'website', 'at']\n",
      "after removing stopword tokens: ['might', 'not', 'come', 'back', 'website']\n",
      "tweet: they is not happy with the service at all\n",
      "before removing stopword tokens: ['they', 'is', 'not', 'happy', 'with', 'the', 'service', 'at', 'all']\n",
      "after removing stopword tokens: ['not', 'happy', 'service']\n",
      "预处理后的数据: [['aww', 'not', 'get', 'home', 'church', 'not', 'go', 'today'], ['not', 'happy', 'table'], ['might', 'not', 'come', 'back', 'website'], ['not', 'happy', 'service']]\n",
      "单词表: {'not': 1, 'happy': 2, 'aww': 3, 'get': 4, 'home': 5, 'church': 6, 'go': 7, 'today': 8, 'table': 9, 'might': 10, 'come': 11, 'back': 12, 'website': 13, 'service': 14}\n",
      "文本的单词索引序列: [[3, 1, 4, 5, 6, 1, 7, 8], [1, 2, 9], [10, 1, 11, 12, 13], [1, 2, 14]]\n",
      "(4, 10)\n",
      "填充后文本的单词索引序列: [[ 0  0  3  1  4  5  6  1  7  8]\n",
      " [ 0  0  0  0  0  0  0  1  2  9]\n",
      " [ 0  0  0  0  0 10  1 11 12 13]\n",
      " [ 0  0  0  0  0  0  0  1  2 14]]\n"
     ]
    }
   ],
   "execution_count": 584
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNag92UMCy1GtgfJ+zFFU1",
   "collapsed_sections": [],
   "name": "DUDL_FFN_aboutMNIST.ipynb",
   "provenance": [
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "interpreter": {
   "hash": "f5c9dfabb21bd2a91b63810df81acbdc6b5e617e45414f0ef050ca96090c868f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
